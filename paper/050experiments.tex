% In this section, we present the experimental results that demonstrate the effectiveness of the \proposed framework.
% We first quantitatively evaluate the topic assignment for documents by using their ground-truth topic class labels.
% Then, we also qualitatively  analyses on the output topic taxonomies obtained by hierarchical topic discovery methods.

\subsection{Experimental Settings}
\label{subsec:expset}
\subsubsection{Datasets}
% Copied from OOCD paper
For our experiments, we use two real-world datasets collected from different domains, \textbf{\nyt}\footnote{The news articles are crawled by using https://developer.nytimes.com/} and \textbf{\arxiv}\footnote{The abstracts of \arxiv papers are crawled from https://arxiv.org/}, and they have a two-level hierarchy of topic classes.
Thus, we regard the hierarchies as the ground-truth provided by a human curator, and use them to evaluate the completeness of topic taxonomies.
%The documents were crawled from 26 categories in 5 different sections (for \nyt), and 34 categories in 3 different sections (for \arxiv).
For both the datasets, the number of documents for each topic class is not balanced, and AutoPhrase~\cite{shang2018automated} is used to tokenize raw texts of each document.
The statistics are summarized in Table~\ref{tbl:datastats}.

\begin{table}[t]
\caption{The statistics of the datasets.}
\centering
\small
%\resizebox{0.99\linewidth}{!}{%
\begin{tabular}{c|cccc}
    \toprule
    Corpus & Avg-Length & \#Topics & \#Documents & \#Terms  \\\midrule
    \textbf{\nyt} & 739.8 & 5 $\rightarrow$ 26 & \ \ 13,081 & 23,245 \\
    \textbf{\arxiv} & 123.5 & 3 $\rightarrow$ 48 & 230,018 & 24,148 \\\bottomrule
\end{tabular}
%}
\label{tbl:datastats}
\end{table}

\begin{table}[t]
\caption{The topic classes deleted from the original topic hierarchy. ($\rightarrow\ast$) denotes all the sub-topics.}
\label{tbl:partialtaxo}
\centering
\resizebox{0.99\linewidth}{!}{%
\begin{tabular}{cll}
\toprule
 & {\textbf{\nyt}} & {\textbf{\arxiv}} \\\midrule
$\taxoa$ & \textit{arts}($\rightarrow\ast$) & \textit{physics}($\rightarrow\ast$) \\\midrule
$\taxob$ & \textit{arts}$\rightarrow$\textit{movies} & \textit{cs}$\rightarrow$\textit{AI,CV,DC,GT,IT} \\
         & \textit{business}$\rightarrow$\textit{stocks} & \textit{math}$\rightarrow$\textit{CA,DS,GR,RT} \\
         & \textit{politics}$\rightarrow$\textit{abortion,budget,insurance} & \textit{physics}$\rightarrow$\textit{chem-ph,gen-ph} \\
         & \textit{sports}$\rightarrow$\textit{baseball,hockey} & \hspace{33pt} \textit{plasm-ph} \\\midrule
$\taxoc$ & \textit{arts}$\rightarrow$\textit{music} & \textit{cs}($\rightarrow\ast$) \\ 
         & \textit{politics}$\rightarrow$\textit{gun control,military} & \textit{math}$\rightarrow$\textit{CO,DG,PR} \\
         & \textit{science}($\rightarrow\ast$) &\textit{physics}$\rightarrow$\textit{atom-ph,flu-dyn} \\
         & \textit{sports}$\rightarrow$\textit{football,soccer} & \\
\bottomrule
\end{tabular}
}
\end{table}

To investigate the effect of an initial topic hierarchy, we consider three scenarios using different partial topic hierarchies with different completeness.
Each partial hierarchy is generated by randomly deleting a few topics from the ground-truth topic hierarchy:
(i) $\taxoa$ deletes only a single first-level topic (and all of its second-level sub-topics), (ii) $\taxob$ drops some of the second-level topics, and (iii) $\taxoc$ does for both levels.
The deleted topics are listed in Table~\ref{tbl:partialtaxo}.

\subsubsection{Baseline Methods}
We consider several methods that are capable of constructing a topic taxonomy (or discovering hierarchical topics) as the baselines.
They can be categorized as either unsupervised methods using only an input corpus, or weakly supervised methods initiated with a given topic hierarchy.

\begin{itemize}
    \item \textbf{\hlda}~\cite{blei2003hierarchical}: Hierarchical latent Dirichlet allocation.
    The document generation process is modeled by selecting a path from the root to a leaf and sampling its words along the path. 
    
    \item \textbf{\taxogen}~\cite{zhang2018taxogen}: The unsupervised framework for topic taxonomy construction. 
    To identify term clusters, it employs the text embedding space, optimized by SkipGram~\cite{mikolov2013distributed}.
    %The number of child nodes is manually set, as done in~\cite{zhang2018taxogen,shang2020nettaxo}.
    
    % \item \textbf{\weshclass}~\cite{meng2019weakly}, \textbf{\taxoclass}~\cite{shen2021taxoclass}:  Hierarchical text classification methods that train their classifier in a weakly supervised setting.
    % The hierarchy of topic surface names is utilized to generate pseudo-documents~\cite{meng2019weakly} or pseudo-labels~\cite{shen2021taxoclass}, which are used for training the classifier.
    
    \item \textbf{\josh}~\cite{meng2020hierarchical}: Hierarchical text embedding technique to mine the set of relevant terms for each given topic. 
    It finds the topic terms based on the directional similarity.
    %During the optimization, it expands the set of topic terms based on the directional similarity, derived from the vMF distribution.
    
    \item \textbf{\corel}~\cite{huang2020corel}: Seed-guided entity taxonomy expansion method.
    It first expands only the topic names based on its relation classifier, then retrieves the topic terms based on the relevance in the embedding space induced by SkipGram~\cite{mikolov2013distributed}.
    
    \item \textbf{\proposed}: The proposed framework for topic taxonomy completion, which finds out novel sub-topic clusters to expand the topic taxonomy in a hierarchical manner.
\end{itemize}
Note that \corel directly learns the ``is-a'' relation of the entity (i.e., topic name) pairs in the given topic hierarchy to discover novel entity pairs.
On the contrary, \proposed implicitly infers the relation at the cluster-level based on its recursive clustering.
Furthermore, \corel mines the topic terms solely based on the embedding similarity, whereas \proposed additionally considers the representativeness in the sub-corpus relevant to the topic, as described in Equation~\eqref{eq:sigscore}.

% \subsubsection{Evaluation Scenarios}
% \label{subsubec:evalscenario}
% We use the Davies-Bouldin index (\dbi)~\cite{davies1979cluster} to assess the quality of clustering results~\cite{zhang2018taxogen}.
% It assign the maximum similarity scores as bla.
% We measure the normalized mutual information (NMI) score between the ground-truth topic labels and the obtained cluster indices.

\input{051humaneval}

\subsection{Quantitative Evaluation}
\label{subsec:doccluster}
\subsubsection{Human evaluation on the quality of topic taxonomy}
\label{subsubsec:humaneval}
First, we assess the quality of topic taxonomies by using human domain knowledge. 
To this end, we recruit 10 doctoral researchers as evaluators, and ask them to perform two tasks that examine the following aspects of a topic taxonomy.\footnote{They are allowed to use web search engines when encountering unfamiliar terms.}
\textbf{(i) Term coherency} indicates how strongly the terms in a topic node are semantically coherent.
Similar to previous topic model evaluations~\cite{xie2015incorporating, shang2020nettaxo}, the top-10 terms of each topic node are presented to human evaluators, and they are requested to identify how many terms are relevant to their common topic (or center term).
The coherency is defined by the ratio of the number of relevant terms over the total number of presented terms.
\textbf{(ii) Topic completeness} quantifies how completely the set of topic nodes covers the ground-truth topics.
For each level, every topic name in the ground-truth topic hierarchy is given as a query, and the set of output topic nodes becomes the support set. 
Human evaluators are asked to rate the score $\in[0,1]$ how confidently the query belongs to one of the topics in the support set (i.e., similarity with the semantically closest support topic).
The completeness is defined by the average score for all the queries. 

In Table~\ref{tbl:humaneval}, \proposed significantly outperforms all the baselines in terms of both the measures.\footnote{We first test the inter-rater reliability on ranks of the methods. We obtain the Kendall coefficient of 0.96/0.91 (\nyt) and 0.94/0.89 (\arxiv) respectively for the coherence and completeness, which indicates the consistent assessment of the 10 evaluators.}
For topic completeness, the weakly supervised methods beat the unsupervised methods by a large margin, because their output topic taxonomy at least covers all the topics in the given topic hierarchy.
Notably, \proposed gets higher scores than \josh and \corel, which implies that it more accurately discovers ground-truth topics deleted from the full hierarchy.
In addition, \proposed is ranked first for the term coherency, as it captures each term's representativeness in the topic-relevant documents as well as its semantic relevance to the target cluster.

\subsubsection{Weakly supervised document classification using topic taxonomy}
\label{subsubsec:weshclass}
Next, we indirectly evaluate each output topic taxonomy by making use of a downstream task that takes a topic taxonomy as its input.
We compare the performance of \weshclass~\cite{meng2019weakly}, a weakly supervised hierarchical text classifier trained by using only unlabeled documents and the hierarchy of target classes (with the class-specific keywords), rather than document-level class labels.
Specifically, we use the topic taxonomy obtained by \proposed and the baseline methods as the hierarchy of target classes, and its top-10 topic terms serve as the class-specific keywords.
We measure the normalized mutual information (NMI) between predicted document topic labels and ground-truth ones in terms of clustering, as well as MacroF1 and MicroF1 in terms of classification.\footnote{In case of topic taxonomies generated by weakly supervised methods, to measure the F1 scores based on document-level topic class labels, we manually find the one-to-one mapping from identified novel topic nodes to the deleted ground-truth topic classes.}

Table~\ref{tbl:docclass} reports that \weshclass achieves the best NMI and F1 scores when being trained using the output topic taxonomy of \proposed.
The final classification performance of \weshclass is mainly affected by (i) the keyword (i.e., top-10 terms) coverage for each topic class and (ii) the topic coverage for the entire text corpus.
As analyzed in Section~\ref{subsubsec:humaneval}, \proposed generates more complete topic taxonomies compared to \josh and \corel, which helps \weshclass to learn the discriminative features for a larger number of ground-truth topic classes in each dataset. 
Besides, since the topic terms retrieved by \proposed captures additionally the representativeness in the topic-relevant documents, they become more informative and accurate class-specific keywords for training \weshclass, which eventually leads to better performances.
%\josh can only find topic terms of the known topics given in an initial topic hierarchy, so it is not able to predict the topic classes that do not exist in the given hierarchy.
%\corel by, and it fails to effectively find out missing topic nodes, which makes the final performance worse.
In conclusion, the higher quality topic taxonomy of \proposed can provide much more useful supervision for the downstream task on unlabeled documents.

\input{052downstream}
\input{054novdetection}

% \subsubsection{Evaluation on document-topic assignment}
% \label{subsubsec:doccluster}
% In case of the hierarchical topic discovery methods that obtain the document-level topic assignment variables as well (i.e., \hlda, \taxogen, and \proposed), we measure its consistency between the assignment and their actual topic labels.
% That is, we evaluate the topic discriminative power of the output topic taxonomy.
% Simlar to Section~\ref{subsubsec:weshclass}, we measure the clustering NMI and NMI*, each of which considers all the documents and. known-topic documents, respectively. 

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{FIG/embspace.pdf}
    \caption{The embedding space of \proposed, which finds multiple sub-topic term clusters (Left) and discriminates between known-topic and novel-topic terms, respectively colored in blue and red (Right). Best viewed in color.}
    \label{fig:embspace}
\end{figure}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{FIG/taxocom_3.pdf}
    \caption{The output topic taxonomy of \proposed, where $\taxoc$ is given as the initial topic hierarchy. Double-lined boxes represent the newly inserted topic nodes. Dataset: \nyt (Upper) and \arxiv (Lower).}
    \label{fig:outputtaxo}
\end{figure*}

\subsubsection{Binary discrimination between known-topic and novel-topic documents}
\label{subsubsec:noveltyeval}
For each partial topic hierarchy, we provide an ablation analysis on the novelty detection performance of \proposed, to validate the effectiveness of the embedding techniques:
\textbf{local embedding} (LE) and \textbf{keyword-guided discriminative embedding} (DE).
Note that there do not exist term-level novelty labels, we instead use document-level novelty labels indicating whether a document belongs to one of the deleted ground-truth topic classes or not.
In other words, we indirectly evaluate the capability of novel topic detection based on the topic assignment of documents, obtained by Equation~\eqref{eq:docassign}.
We consider precision, recall, and F1 score as the evaluation metrics.
For a fair comparison, we select the optimal hyperparameter value $\beta\in\{1.5, 2.0, 2.5, 3.0\}$ to determine the novelty threshold (in Equation~\eqref{eq:novscore}) of the ablated methods.

In Table 5, \proposed that adopts both LE and DE performs the best for distinguishing novel-topic documents from known-topic counterparts.
Particularly, DE considerably improves the novelty detection performance, by collecting the given topic names from the sub-tree rooted at each node and utilizing them as the keywords for the topic, as discussed in Section~\ref{subsubsec:disemb}.
In summary, LE and DE optimize the embedding space further discriminative among known sub-topics, and it is helpful to enhance the binary discrimination between known and novel sub-topics as well.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{FIG/case_study.pdf}
    \caption{Topic terms retrieved by \corel and \proposed.}
    \label{fig:casestudy}
\end{figure}

\subsection{Qualitative Analysis}
\label{subsec:qualanal}
\subsubsection{Case study on topic taxonomy}
\label{subsubsec:casestudy}
We qualitatively examine the output taxonomy of \proposed.
Figure~\ref{fig:outputtaxo} shows that \proposed expands the topic taxonomy while preserving the high-level design of the given topic hierarchy $\taxoc$.
To be specific, in case of \nyt, it successfully identifies not only the first-level missing topic \textit{science} but also the second-level ones including \textit{music}, \textit{football}, and \textit{soccer}.
We observe that several center terms of novel topic nodes do not exactly match with the ground-truth topic names, such as \textit{spacecraft}-\textit{cosmos} (\nyt), \textit{data processing}-\textit{computer science} (\arxiv), and \textit{fluid mechanics}-\textit{fluid dynamics} (\arxiv).
Nevertheless, it is obvious that they represent the same conceptual topic of some documents in the text corpus, in light of the terms assigned to them.

Furthermore, we compare the topic nodes (and their anchor terms) identified by \proposed and \corel.
In Figure~\ref{fig:casestudy}, several topic terms of \corel are too general to belong to the topic (marked in red), whereas \proposed selectively filters the topic-relevant terms by taking advantage of topic sub-corpus.
In Table~\ref{tbl:casestudy}, \corel often finds redundant topics semantically overlapped with a known sub-topic (\ding{34}), or just novel entities in the ``is-a'' relation rather than a topic class of documents in the corpus ($\Motimes$).
Especially, \corel fails to find any of the first-level missing topics due to the lack of given ``is-a'' relations.
%Note that \josh is not able to identify novel topics at all.
In contrast, \proposed effectively completes the latent topic structure of the text corpus by discovering novel topics that semantically deviate from the known topics.

\input{055casestudy}

\subsubsection{Visualization of discriminative embedding space}
\label{subsubsec:embvis}
We also visualize our spherical embedding space for the \nyt dataset, when $\taxoc$ is given as the partial topic hierarchy.
Figure~\ref{fig:embspace} shows the embedding space plotted by t-SNE~\cite{van2008visualizing} for (i) the root node and (ii) the \textit{sports} node.
In the left figures, the anchor terms assigned in multiple sub-topics are marked in different colors, while each center term and the non-anchor terms are plotted as black asterisks and white circles, respectively.
We annotate the center term of the novel clusters that correspond to the novel topic nodes (i.e., \textit{science}, \textit{football}, and \textit{soccer}) presented in Section~\ref{subsubsec:casestudy}.
On the other hand, the right figures illustrate the binary discrimination between known-topic and novel-topic terms, determined based on the novelty score (Equation~\eqref{eq:novscore}).
Our confidence-based novelty score is effective to detect novel-topic terms (and their dense clusters) clearly distinguishable from known-topic terms in the embedding space.
%In conclusion, for topic taxonomy completion, \proposed performs the hierarchical clustering that recursively tunes the local embedding space and finds out sub-topic clusters.