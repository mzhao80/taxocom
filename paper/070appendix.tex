\subsection{Pseudo-code of \proposed}
\label{subsec:pseudocode}
Algorithm~\ref{alg:overview} describes the overall process of our framework for topic taxonomy completion.
Starting from the root node (Line 2), \proposed recursively performs text embedding and clustering for each topic node (Lines 5 and 6) to find out multiple sub-topic term clusters.
Based on the clustering result, it expands the current topic taxonomy by inserting novel sub-topic nodes (Line 8).
In the end, \proposed outputs the complete topic taxonomy (Line 10).
\input{071algorithm}

\subsection{Implementation Details}
\label{subsec:impdetail}
We implement the main recursive process (Section~\ref{subsec:overview}) and the novelty adaptive clustering (Section~\ref{subsec:clustering}) by using Python,
and the locally discriminative embedding (Section~\ref{subsec:embedding}) is written in C for efficient optimization based on multi-threaded parallel computation.
For the other baselines, we employ the official author codes of \hlda\footnote{https://github.com/joewandy/hlda}, \taxogen\footnote{https://github.com/franticnerd/taxogen}, \josh\footnote{https://github.com/yumeng5/JoSH}, and \corel\footnote{https://github.com/teapot123/CoRel}.

For \textit{novelty adaptive clustering} of \proposed, we set $\beta=1.5, 3.0$ (for the first-level and second-level topic nodes, respectively) in the novelty threshold $\tau_{nov}$ (Equation~\eqref{eq:novscore}), and the significance threshold $\tau_{sig}=0.3$ (Equation~\eqref{eq:termassign}),
without further tuning for each dataset or initial topic hierarchy.
For \textit{locally discriminative embedding} of \proposed, we set the margin $m=0.3$ (Equation~\eqref{eq:josdopt}), and the number of neighbor terms $M=100$ (Section~\ref{subsubsec:locemb}) to retrieve the sub-corpus which are used for tuning the embedding space, i.e., top-$100$ closest term of each center term.
%\footnote{We empirically found that all these hyperparameters hardly affects the final output.}

For all the embedding-based methods (i.e., \taxogen, \josh, \corel, and \proposed) that optimize the Euclidean space or spherical space, we fix the number of negative terms (for each positive term pair) to 2 during the optimization.
For \hlda, we set (i) the smoothing parameter over document-topic distributions $\alpha=0.1$, (ii) the concentration parameter in the Chinese Restaurant Process $\gamma=1.0$, and (iii) the smoothing parameter over topic-word distributions $\eta=1.0$.
For \taxogen, we follow the parameter setting provided by~\cite{zhang2018taxogen};
i.e., the number of child nodes is set to 5.


% \subsection{Baseline Methods}
% \label{subsec:impdetail}
% Table~\ref{tbl:basecomp} presents the comparison of embedding-based baseline methods.

% \begin{table}[thbp]
% \caption{Comparison of embedding-based baseline methods.}
%     \centering
%     \resizebox{0.99\linewidth}{!}{%
%     \begin{tabular}{l|ccPPPP}
%         \toprule
%         \multirow{2}{*}{\textbf{Methods}} & \multirow{2}{*}{\shortstack{\textbf{Given}\\\textbf{Taxo}}} & \multirow{2}{*}{\shortstack{\textbf{Novel}\\\textbf{Topic}}} & \multicolumn{2}{c}{\textbf{Embedding}}& \multicolumn{2}{c}{\textbf{Term Mining}} \\
%         & & & LE & DE & Rel & Rep \\\midrule
%          \taxogen~\cite{zhang2018taxogen} & & & \checkmark  &  & \checkmark & \checkmark \\
%          \josh~\cite{meng2019weakly} & \checkmark & & & \checkmark & \checkmark &  \\
%          \corel~\cite{huang2020corel} & \checkmark  & \checkmark & & & \checkmark & \\
%          \textbf{\proposed} & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark & \checkmark \\\bottomrule
%     \end{tabular}
%     }
%     \label{tbl:basecomp}
% \end{table}

\subsection{Constructed Topic Taxonomy}
\label{subsec:outputtaxo}

\begin{figure}[h]
\centering
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{FIG/taxocom_nyt_1.pdf}  
    \caption{Dataset: \nyt}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{FIG/taxocom_arxiv_1.pdf}
    \caption{Dataset: \arxiv}
\end{subfigure}
\caption{The output taxonomy of \proposed, where $\taxoa$ is given as the initial topic hierarchy. Double-lined boxes represent the novel topic nodes.}
\label{fig:taxoa}
\end{figure}

Figures~\ref{fig:taxoa} and \ref{fig:taxob} show the topic taxonomies generated by \proposed, where $\taxoa$ and $\taxob$ are given as the initial topic hierarchy, respetively.\footnote{The output taxonomies started from $\taxoc$ is presented in Figure~\ref{fig:outputtaxo}.}
The center terms (i.e., topic names) and topic terms are presented without any post-processing such as manual filtering or selection that requires human labor.
Due to the space limit, the figures omit most of the second-level known topic nodes (i.e., the topics already included in $\taxoa$ or $\taxob$), rather focus on newly inserted novel topic nodes.
Note that the initial topic hierarchies are generated by random node drop of the ground-truth topic hierarchy:
a single first-level topic (in case of $\taxoa$) and a portion of second-level topics (in case of $\taxob$) is deleted, as listed in Table~\ref{tbl:partialtaxo}.

We can observe that \proposed effectively detects the deleted topic nodes and places them in the right position within the topic taxonomy.
In other words, the output taxonomies more completely cover the topic structure of each dataset, compared to the initial topic hierarchy, $\taxoa$ or $\taxob$.
The recursive clustering process of \proposed implicitly forces the hierarchical semantic relationship of parent-child node pairs, while clearly distinguishing novel sub-topic clusters from known sub-topic clusters based on the score of how confidently each term belongs to one of the known sub-topics (Equation~\ref{eq:novscore}).
As discussed in Section~\ref{subsubsec:casestudy}, some center terms of identified novel topic nodes do not match with the ground-truth topic names.
In spite of the minor mismatch, we can easily figure out the one-to-one mapping from the novel topics to the deleted ground-truth topics; 
for example, \textit{films}-\textit{movies}, \textit{stock market}-\textit{stocks}, \textit{insurer}-\textit{insurance} (\nyt), and \textit{natural sciences}-\textit{physics}, \textit{accelerator}-\textit{accelerator physics}, \textit{atoms}-\textit{atomic physics} (\arxiv).

\begin{figure}[t]
\centering
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{FIG/taxocom_nyt_2.pdf}  
    \caption{Dataset: \nyt}
\end{subfigure}
\begin{subfigure}{\linewidth}
    \centering
    \includegraphics[width=\linewidth]{FIG/taxocom_arxiv_2.pdf}
    \caption{Dataset: \arxiv}
\end{subfigure}
\caption{The output taxonomy of \proposed, where $\taxob$ is given as the initial topic hierarchy. Double-lined boxes represent the novel topic nodes.}
\label{fig:taxob}
\end{figure}

% \subsection{Experimental Results}
% \label{subsec:addexp}
% \subsubsection{Qualitative comparison of top-10 terms}
% \label{subsubsec:qualcomp}

% \subsubsection{Parameter Analysis}
% \label{subsubbsec:paramanal}
% We investigate the performance changes of \proposed, with respect to the novelty threshold $\tau_{nov}$ and the significance threshold $\tau_{sig}$.
% (TO BE WRITTEN)
